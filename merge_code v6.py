import os
import re
import pandas as pd
from bs4 import BeautifulSoup
import numpy as np
import csv
from fuzzywuzzy import fuzz
import time
import csv
import sys
import itertools
from itertools import islice
from collections.abc import Iterable

"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
                                        TO DO BEFORE RUNNING THE CODE

- remember that after first big loop there is a 'load' statement for the 'merged' file that has to be turnoed on or off
  according to the needs
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
PURPOSE: - bring together the firms-brands in "all_matches.xlsx" and the brands-reviews from 
           "Metadata" and "Reviews"
         - compute the tercile portfolios for replication of table 3 in CKB 
STRUCTURE: - it is divided into sections delimited by """""""
           - each section executes a main task and contains detailed comment regarding the functioning
             and rationale of the code
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

# ======================================================================================================================
# The following while-loop is set to prevent 'Error: field larger than field limit (131072)' generated by the large
# amount of iterations generated in eraly versions of the code. We have kept it as a precaution should it be useful in
# the future. In any case it is not computationally intensive.
# ======================================================================================================================
# maxInt = sys.maxsize
# while True:
#     # decrease the maxInt value by factor 10 as long as the OverflowError occurs.
#     try:
#         csv.field_size_limit(maxInt)
#         break
#     except OverflowError:
#         maxInt = int(maxInt / 10)
#
# """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
# • Section 1: Now with the following for-loop we bring together the merged files with metadata and reviews with the
#              firm-brand links
# """""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
# # set directory
# # Mattia's Mac path: /Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Meta_reviews_merged
#
# directory = r'/Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Meta_reviews_merged'
# os.chdir(directory)
#
# # Import file with grocery companies (from here on 'short file') and relative codes into DataFrame and clean it:
# # Mattia's Mac path: /Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/grocery_companies_codes.csv
# sf = pd.read_csv(r'/Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/grocery_companies_codes.csv').drop(
#     columns=['Unnamed: 0'])
# categories = list(sf['main_cat'].unique())
#
# # Create a DatFrame, to introduce the merged data:
# merged = pd.DataFrame(columns=(
# 'Brand', 'Title', 'Public Company', 'Main Category', 'Overall', 'Review Date', 'Review Date (weird encoding)', 'ASIN',
# 'Matched PERMCO', 'Matched PERMNO', 'Matched GVKEY', 'Matched TIC', 'Matched SIC'))
#
# # Open the merged file with metadata and reviews (from here on the 'long file'):
# t0 = time.time()
# for filename in os.listdir(directory): # TODO: check the functioning of this big for-loop
#     name_dataset = os.path.join(directory, filename)  # .path.join is a method in Python join one or more path
#     # components intelligently. This method concatenates various path
#     # components with exactly one directory separator (‘/’) following
#     # each non-empty part except the last path component.
#     print("Executing dataset: " + name_dataset)
#
#     with open(os.path.join(directory, filename), 'r', encoding='utf-8', newline='') as myfile:
#         next(myfile)
#         category = str(filename).replace('.csv', '')
#         for row in csv.reader(myfile, delimiter=','):
#             # ==========================================================================================================
#             #   Now, match the brand name from 'long file' with the brands from the 'short file' that have the same
#             #   category. 'brand_score' contains for each brand in the 'long file':
#             #       - in column 1, the name of each brand in the 'short file' that has the same category
#             #       - in column 2, the fuzzy ratio between the brand in the 'long file' and consecutevely each brand
#             #                      in the 'short file' that has the same category
#             # ==========================================================================================================
#
#             brand_score = pd.DataFrame(columns=('Brand', 'Score'))
#             for element in sf.itertuples():
#                 if element[3] == category:
#                     to_append = [element[1], fuzz.ratio(row[2].lower(), element[1].lower())]
#                     prov = pd.DataFrame(to_append, index=brand_score.columns)
#                     brand_score = brand_score.append(prov.transpose(), ignore_index=True)
#
#             # ==========================================================================================================
#             #   In the 'long file' there are way more brands than in the 'short file'. To avoid fuzzywuzzy from
#             #   matching 'Wilton' with 'Tim Horton' because 'Wilton' is not in the 'short file' we impose a minimum
#             #   degree of similarity from here on:
#             # ==========================================================================================================
#
#             if int(max(list(brand_score['Score']))) >= 80:
#                 # ======================================================================================================
#                 #   The following code retrieves the specific entry in the 'short file' that contains the 'Brand' of
#                 #   the row from the 'long file' we are looking at. The final output is
#                 #       -   'merged', a dataset containing for for each brand-product, all the ratings received by the
#                 #                     product and the respective dates, plus a bunch of other info.
#                 #                     The columns are: brand, title, publi company, main category, overall (rating),
#                 #                                      review date, review date (UNIX), asin, permno, permco, gvkey,
#                 #                                      sic, tic
#                 # ======================================================================================================
#                 index = list(sf['Brand']).index(
#                     brand_score['Brand'][list(brand_score['Score']).index(max(list(brand_score['Score'])))])
#                 print(str(index) + ' / ' + row[2] + ' / ' + brand_score['Brand'][
#                     list(brand_score['Score']).index(max(list(brand_score['Score'])))] + ' / ' + str(
#                     max(list(brand_score['Score']))))
#
#                 to_append = [row[2], row[1], sf['Public_company'][index], category, row[5], row[7], row[9], row[4],
#                              sf['matched_permco'][index], sf['matched_permno'][index], sf['matched_gvkey'][index],
#                              sf['matched_tic'][index], sf['matched_sic'][index]]
#
#                 prov = pd.Series(to_append, index=merged.columns)
#                 merged = merged.append(prov, ignore_index=True)
#
# t1 = time.time()

# ======================================================================================================================
# !!! IMPORTANT !!!: saving the 'merged' file, TURN ON only if want to OVERWRITE the FILE already created in the past
# ======================================================================================================================
# merged.to_csv(r"D:\Dropbox (IESE)\Useful Material\Amazon Project\Data\Merged Brands (longshort)\grocery_m_ls.csv",
#               index=False)

# ======================================================================================================================
# load file '..._m_ls' created just above
# Mattia's Mac path
# /Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Merged Brands (longshort)/grocery_m_ls.csv
# Mattia's PC path
# D:\Dropbox (IESE)\Useful Material\Amazon Project\Data\Merged Brands (longshort)\grocery_m_ls.csv
# ======================================================================================================================

merged = pd.read_csv(r"/Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Merged Brands (longshort)/"
                     r"grocery_m_ls.csv")

# 'permnos' is a list of all the permnos that have reviews, found in 'merged'
permnos = list(merged['Matched PERMNO'].unique())

"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
• Section 2: The following piece of code computes the ratings for each permno and each month of available data.
             It also saves the number of reviews per permno and month.
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

#   The code below is to order the months in ascending order, which hopefully will come in handy later on.
months = []
m = []
y = []
# this loop saves all months of available data, in ascending order.
for entry in list(merged['Review Date'].unique()):
    # date takes the month and the year from US format dates
    date = entry[:2] + '/' + entry[-4:]
    if date not in months:
        months.append(date)
        m.append(date.split('/')[0])
        y.append(date.split('/')[1])
y, m = (list(t) for t in zip(*sorted(zip(y, m))))

# here months is overwritten to append all the mm/yyyy couples
months = []
for i in range(len(y)):  # length is each year x 12
    months.append(y[i] + m[i])

#   The following two DataFrames will contain:
#       - average_overall, the average rating of a brand for each month
#       - number_reviews, the number of reviews for that brand on that month

col_start = ['Matched PERMNO']
average_overall = pd.DataFrame(columns=col_start + months)
number_reviews = pd.DataFrame(columns=col_start + months)

# ======================================================================================================================
# the following for-loop uses 'brands', which is the list of brands built before from all the permnos for which we have
# reviews. The intermediate output is:
#   - 'reviews', a temporary dataset that has for each permno, the rankings, date and UNIX date from each review
#   - 'temp_average', a temporary dataset that has for each permno the average reviews in each month,
#                     if that month there are no reviews print a 'nan'
#   - 'temp_number', a temporary dataset that has for each permno the number (count) of reviews in each month
# the final output is:
#   - 'average_overall', is a dataset containing the average rating for each permno in each month of the sample
#   - 'number_reviews', is a dataset containing the number of ratings (therefore also the reviews, although we do not
#                       have reviews here) for each permno in each month of the sample.
# ======================================================================================================================

for permno in permnos:
    reviews = pd.DataFrame(columns=('Overall', 'Normal Date', 'Weird Date'))
    # 'row[1]' is the brand from 'merged'
    for row in merged.itertuples():
        if row[9] == permno:
            a = []
            b = []
            c = []
            a.append(row[5])
            b.append(row[6])
            c.append(row[7])
            prov = pd.DataFrame(list(zip(a, b, c)), columns=('Overall', 'Normal Date', 'Weird Date'))
            reviews = reviews.append(prov, ignore_index=True)

    temp_average = [permno]
    temp_number = [permno]
    # months contains mm/yyyy
    for month in months:
        number_rate = 0
        total = 0
        # row[2] is the column with review dates in US format
        for row in reviews.itertuples():
            # date gets the month and the year from 'reviews'
            date = row[2][-4:] + row[2][:2]
            # 'count' keeps track of number of reviews in each month
            # 'total' sums up tha ratings (using row[1]) for each brand in each month
            if date == month:
                number_rate += 1
                total += float(row[1])
        if number_rate != 0:
            temp_average.append(total / number_rate)
        else:
            temp_average.append('nan')
        temp_number.append(number_rate)

    prov1 = pd.Series(temp_average, index=average_overall.columns)
    average_overall = average_overall.append(prov1, ignore_index=True)
    prov2 = pd.Series(temp_number, index=number_reviews.columns)
    number_reviews = number_reviews.append(prov2, ignore_index=True)
    print(permno)

"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
• Section 3: At this point we should have two dataframes
             -  one with the monthly average ratings
             -  the other with the count of review per month for ALL permnos.
             With this information about the permnos we can generate the 'abnormal ratings' measure
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

# the following for-loop gives as final output:
#   - 'abnormal_matrix', which includes the abnormal rating for each permno and month
#                       (i.e. the deviation in a month from the average rating in the previous 12 months).
#                       the first column is the permno and following columns are the dates.
# months[12:] creates a delay to consider from the 13th month onward
abnormal_matrix = pd.DataFrame(
    columns=col_start + months[12:])  # Here we assume we'll have observations for all months.
for row in average_overall.itertuples():
    # row[1] is the permno from average_overall
    abnormal = [row[1]]
    # row[14:] takes the length from the 13th month onwards until the end of the sample period
    for i in range(len(row[14:])):
        grand_total = 0
        number_rate = 0
        for x in range(1, 13):  # .columns does not count the index column.
            # i carries time forward and x takes care of the lag
            # 'grand_total', computes for each brand the number of ratings in a month * the average rating in that month
            # 'number_rate', computes for each brand the number of ratings in a month
            if row[14 + i - x] != 'nan':
                grand_total += number_reviews[average_overall.columns[13 + i - x]][
                                   list(number_reviews['Matched PERMNO']).index(row[1])] * row[14 + i - x]
                number_rate += number_reviews[average_overall.columns[13 + i - x]][
                    list(number_reviews['Matched PERMNO']).index(row[1])]
        if number_rate != 0:
            average = grand_total / number_rate
        else:
            average = 0
        if row[14 + i] == 'nan':
            abnormal.append('nan')
        else:
            abnormal.append(row[14 + i] - average)

    prov = pd.Series(abnormal, index=abnormal_matrix.columns)
    abnormal_matrix = abnormal_matrix.append(prov, ignore_index=True)

"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
• Section 4: Now, we compute the tercile portfolios based on abnormal ratings
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

# 'final_portfolio', keeps track, each month, of the permnos that are in each portfolio
# 'final_portfolio_c', keeps track of the count of the reviews each permno of each tercile has received in a given month

final_portfolio = pd.DataFrame(columns=('Month', 'T1', 'T2', 'T3'))
final_portfolio_c = pd.DataFrame(columns=('Month', 'T1', 'T2', 'T3'))

all_permnos = list(abnormal_matrix['Matched PERMNO'])
print(len(all_permnos))
abn_matr_trans = abnormal_matrix.transpose()  # TODO: this should be deleted, just for visual check
for month in islice(abnormal_matrix.transpose().itertuples(), 1,
                    None):  # TODO: check with Carles that month should actually pick up
    #       the months, because in the original code it is picking
    #       up the PERMNOs first
    abnormal = []  # For each month this list will contain the 'abnormal ratings' (i.e. devietions from
    # past 12 months mean) not equal to 'nan'
    temp_permnos = []  # and this one the corresponding permons.
    counts_ratings = []  # This will get the number of counts (i.e. ratings) to do the weighting later on.
    i = -1  # TODO: ask Carles if he remembers exactly what this is for, probably to start from 0
    MONTHYEAR_ABNORMAL_MATRIX = 0
    for x in month[1:]:  # here we get for each month the abnormal ratings across all permnos. Each row has in first
        # column the month and in the following columns, each entry represent the abnormal
        # rating in that month for a specific permno
        i += 1
        if str(x) != 'nan':
            try:
                abnormal.append(float(x))  # append the abnormal rating
                temp_permnos.append(all_permnos[i])  # append the correponding permno
                counts_ratings.append(
                    number_reviews[str(month[MONTHYEAR_ABNORMAL_MATRIX])][
                        list(number_reviews['Matched PERMNO']).index(all_permnos[i])])
            except:
                print("There was a 'NaN'")

    # sort and divide the observations in three roughly equal groups to build the terciles
    if len(abnormal) != 0:
        abnormal, temp_permnos, counts_ratings = (list(t) for t in
                                                  zip(*sorted(zip(abnormal, temp_permnos, counts_ratings))))
        div = round(len(abnormal) / 3)

        t1 = temp_permnos[:div]
        t2 = temp_permnos[div:2 * div]
        t3 = temp_permnos[2 * div:]
        portfolios = [month[MONTHYEAR_ABNORMAL_MATRIX], t1, t2, t3]  # TODO: check how many portfolios do not have one
        #       value per tercile. This might cause problems
        #       in the econometric analysis
        prov = pd.Series(portfolios, index=final_portfolio.columns)
        final_portfolio = final_portfolio.append(prov, ignore_index=True)

        ct1 = counts_ratings[:div]
        ct2 = counts_ratings[div:2 * div]
        ct3 = counts_ratings[2 * div:]
        portfolios_c = [month[MONTHYEAR_ABNORMAL_MATRIX], ct1, ct2, ct3]
        prov_c = pd.Series(portfolios_c, index=final_portfolio_c.columns)
        final_portfolio_c = final_portfolio_c.append(prov_c, ignore_index=True)

# save final_portfolio and final_portfolio_c for debugging
final_portfolio.to_csv(
    r"/Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Auxiliary/final_portfolio.csv", index=False)
print("final_portfolio done")

final_portfolio_c.to_csv(
    r"/Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Auxiliary/final_portfolio_c.csv", index=False)
print("final_portfolio_c done")

"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
• Section 5: In this section we bring together the tercile portfolios and excess returns
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

# Mattia's OS path:   /Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Auxiliary/msf_riskfree_brands.csv
# Mattia's Windows path:   D:\Dropbox (IESE)\Useful Material\Amazon Project\Data\Auxiliary\msf_riskfree_brands.csv
financial_brands = pd.read_csv(
    r"/Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Auxiliary/msf_riskfree_brands.csv")

# Load portfolios for debugging
final_portfolio = pd.read_csv(
    r"/Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Auxiliary/final_portfolio.csv")
final_portfolio_c = pd.read_csv(
    r"/Users/mattia/Dropbox (IESE)/Useful Material/Amazon Project/Data/Auxiliary/final_portfolio_c.csv")


# the idea now could be to go in 'final_portfolio', extract each month the permnos that are in each portfolio and go
# into 'financial_brands' and join if yyyymm and permnos are the same in both. We could have for output a df where:
# -     the first column we have yyyymm
# -     the following columns T1(company) T1(permno) T2(company) T2(permno) T3(company) T3(permnos)

# We define two functions useful in the next for-loop:
#   -   flatten: used to 'flatten' lists (i.e. make nested lists into a single-level list)
#   -   keep_running: used to keep the program running if there are no permnos in a cell
def flatten(lis):
    for item in lis:
        if isinstance(item, Iterable) and not isinstance(item, str):
            for x in flatten(item):
                yield x
        else:
            yield item


def keep_running(name):
    if name != '[]':
        return [float(i) for i in name[1:-1].split(', ')]
    else:
        pass

# The following for-loop divides the terciles into three different DataFrames, each containing the moonth and the
# permnos pertaining to that tercile in that month. The main output is:
#   -   date_terc_permnos1, contains two columns: 'Month' and 'T1'. The date (YYYYMM) and the permnos in the first
#                           tercile that month
#   -   date_terc_permnos2, contains two columns: 'Month' and 'T2'. The date (YYYYMM) and the permnos in the second
#                           tercile that month
#   -   date_terc_permnos3, contains two columns: 'Month' and 'T3'. The date (YYYYMM) and the permnos in the third
#                           tercile that month

date_terc_permnos1 = pd.DataFrame()
date_terc_permnos2 = pd.DataFrame()
date_terc_permnos3 = pd.DataFrame()
MONTH = 1
COLUMN_T1 = 2
COLUMN_T2 = 3
COLUMN_T3 = 4
# the for-loop goes over each row in 'final_portfolio', goes into each column, and extracts the date and the permnos
# and separates them into the three files
for row in final_portfolio.itertuples():
    i_T1 = []
    i_T2 = []
    i_T3 = []
    for permnos in row[COLUMN_T1:]:
        if permnos in row[COLUMN_T1]:
            permnos = keep_running(permnos)
            i_T1.append(permnos)
            i_T1 = list(flatten(i_T1))
            date = sorted([row[MONTH]] * len(i_T1))
            prov_T1 = pd.Series(i_T1, name="T1")
            prov_date = pd.Series(date, name="Month")
            prov_date_terc_permnos1 = pd.concat([prov_date, prov_T1], axis=1)
            date_terc_permnos1 = date_terc_permnos1.append(prov_date_terc_permnos1, ignore_index=True)
        elif permnos in row[COLUMN_T2]:
            permnos = keep_running(permnos)
            i_T2.append(permnos)
            i_T2 = list(flatten(i_T2))
            date = sorted([row[MONTH]] * len(i_T2))
            prov_T2 = pd.Series(i_T2, name="T2")
            prov_date = pd.Series(date, name="Month")
            prov_date_terc_permnos2 = pd.concat([prov_date, prov_T2], axis=1)
            date_terc_permnos2 = date_terc_permnos2.append(prov_date_terc_permnos2, ignore_index=True)
        else:
            permnos = keep_running(permnos)
            i_T3.append(permnos)
            i_T3 = list(flatten(i_T3))
            date = sorted([row[MONTH]] * len(i_T3))
            prov_T3 = pd.Series(i_T3, name="T3")
            prov_date = pd.Series(date, name="Month")
            prov_date_terc_permnos3 = pd.concat([prov_date, prov_T3], axis=1)
            date_terc_permnos3 = date_terc_permnos3.append(prov_date_terc_permnos3, ignore_index=True)
print("date_terc_permnos done")
# TODO: date_terc_c... have different number of rows (not by much but, <10 for grocery) is this a problem?

# This for-loop works in the same fashion as the previous for-loop but gets the count for how many reviews
# each permno in each tercile has had in a specific month.
date_terc_c1 = pd.DataFrame()
date_terc_c2 = pd.DataFrame()
date_terc_c3 = pd.DataFrame()
for row in final_portfolio_c.itertuples():
    i_T1 = []
    i_T2 = []
    i_T3 = []
    for counting in row[COLUMN_T1:]:
        if counting in row[COLUMN_T1]:
            counting = keep_running(counting)
            i_T1.append(counting)
            i_T1 = list(flatten(i_T1))
            date = sorted([row[MONTH]] * len(i_T1))
            prov_T1 = pd.Series(i_T1, name="T1")
            prov_date = pd.Series(date, name="Month")
            prov_date_terc_c1 = pd.concat([prov_date, prov_T1], axis=1)
            date_terc_c1 = date_terc_c1.append(prov_date_terc_c1, ignore_index=True)
        elif counting in row[COLUMN_T2]:
            counting = keep_running(counting)
            i_T2.append(counting)
            i_T2 = list(flatten(i_T2))
            date = sorted([row[MONTH]] * len(i_T2))
            prov_T2 = pd.Series(i_T2, name="T2")
            prov_date = pd.Series(date, name="Month")
            prov_date_terc_c2 = pd.concat([prov_date, prov_T2], axis=1)
            date_terc_c2 = date_terc_c2.append(prov_date_terc_c2, ignore_index=True)
        else:
            counting = keep_running(counting)
            i_T3.append(counting)
            i_T3 = list(flatten(i_T3))
            date = sorted([row[MONTH]] * len(i_T3))
            prov_T3 = pd.Series(i_T3, name="T3")
            prov_date = pd.Series(date, name="Month")
            prov_date_terc_c3 = pd.concat([prov_date, prov_T3], axis=1)
            date_terc_c3 = date_terc_c3.append(prov_date_terc_c3, ignore_index=True)
print("date_terc_c done")

# the next three mergeings bring together each permno in each month, divided by terciles, with the corresponding
# excess returns for the Fama-French_Carhartt four factor model regressions.
fin_permnos_terc1 = date_terc_permnos1.merge(financial_brands, how='outer', right_on=['date_short', 'matched_permno'],
                                             left_on=['Month', 'T1']).merge(date_terc_c1, how='outer',
                                                                            right_on=['Month', 'T1'],
                                                                            left_on=['Month', 'T1'])
fin_permnos_terc2 = date_terc_permnos2.merge(financial_brands, how='outer', right_on=['date_short', 'matched_permno'],
                                             left_on=['Month', 'T2']).merge(date_terc_c2, how='outer',
                                                                            right_on=['Month', 'T2'],
                                                                            left_on=['Month', 'T2'])
fin_permnos_terc3 = date_terc_permnos3.merge(financial_brands, how='outer', right_on=['date_short', 'matched_permno'],
                                             left_on=['Month', 'T3']).merge(date_terc_c3, how='outer',
                                                                            right_on=['Month', 'T3'],
                                                                            left_on=['Month', 'T3'])
print("merge done")

"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
• Section 6: Build the "Long/Short" portfolio
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

long_short_portfolio = pd.DataFrame()
RETX = 12
for row in fin_permnos_terc1.iterrows():
    excess_return = row[RETX]



print("The code has finished")

"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
                                                    END OF CODE
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

# ======================================================================================================================
# ======================================================================================================================
#           ---------------------------   SCRAP CODE REPOSITORY FROM HERE ON   ---------------------------
# ======================================================================================================================
# ======================================================================================================================


# ======================================================================================================================
#   In this section we bring together the tercile portfolios and escess returns
# ======================================================================================================================


# date_terciles = dateT1.merge(dateT2, on = 'Month', how = 'outer').merge(dateT3, on = 'Month', how = 'outer')

# fin_brand_terc1 = financial_brands.merge(date_terciles, )

# elements = el.split(',')

# row[1] =

# for el in row[1:3]:
#     elements = list(el)
#     if elements in row[1]:
#         for i in elements:
#             prov_dateT1 = pd.Series(i, index = date_terc_brands1['T1'])
#             date_terc_brands1 = date_terc_brands1.append(row[0], prov_dateT1)
#     elif elements in row[2]:
#         for i in elements:
#             prov_dateT2 = pd.Series(elements, index = date_terc_brands2['T2'])
#             date_terc_brands2 = date_terc_brands2.append(row[0], prov_dateT2)
#     else:
#         prov_dateT3 = pd.Series(elements, index = date_terc_brands3['T3'])
#         dateT3 = dateT3.append(row[0], prov_dateT3)

# date_terc_brands1 = date_terc_brands1.append(dateT1, index = date_terc_brands1.columns)
# date_terc_brands2 = date_terc_brands2.append(dateT2, index = date_terc_brands2.columns)
# date_terc_brands3 = date_terc_brands3.append(dateT3, index = date_terc_brands3.columns)

# with open(r'D:\Dropbox (IESE)\Useful Material\Amazon Project\Data\Meta_reviews_merged\Grocery_m.csv', 'r', encoding = 'utf-8', newline = '') as myfile:
#     next(myfile) # Skips first row
#     for row in csv.reader(myfile, delimiter = ','):
# # ====================================================================================================================
# #
# #   Extract categories (to reduce the number of elements to check with fuzzywuzzy)
# #   Compare 'category' in meta_reviews with 'main_cat' in grocery company codes and pick the most similar one, ie higher score
# #   (in this case it doesn't matter cause we only have 'Grocery' data, but should
# #   speed up the process with the whole sample.)
# #
# #   !!! MAYBE WE CAN SKIP THIS STEP SINCE THE DATASETS ARE IN DIFFERENT FILES EPARATED BY CATEGORIES !!!
# #
# # ====================================================================================================================
#         temp_categories = row[0][1:-1].replace(',' , '').split("'")
#         cat_score = pd.DataFrame(columns = ('Cat', 'Score'))

#         for t_cat in temp_categories:
#             for cat in categories:
#                 to_append = [cat, fuzz.partial_ratio(t_cat.lower(),cat.lower())]
#                 prov = pd.Series(to_append, index = cat_score.columns)
#                 cat_score = cat_score.append(prov, ignore_index = True)

#         category = cat_score['Cat'][list(cat_score['Score']).index(max(list(cat_score['Score'])))]
#         print(category + '  ' + str(max(list(cat_score['Score']))))
